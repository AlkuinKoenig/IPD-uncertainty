---
title: "Isotope deconvolution notebook"
output:
  html_document:
    df_print: paged
    code_folding: show
---

Version 1
Created by Alkuin Koenig <alkuin.koenig@gmx.de>
2023/09/13

### user input
##This version takes relative uncertainties. 
```{r}
library(here)# neat library for relative paths

#path to the excel sheet containing information about the isotope abundancies & their uncertainties (the "key" matrix)
input_key_xlxs = paste0(here::here(),"/data_input/isotope_keys_testfile_AK.xlsx")
#path to the excel sheet containing information about the samples (peak area, sample volume, etc) and uncertainties
input_data_xlxs = paste0(here::here(),"/data_input/sample_data_testfile_AK_rel.xlsx")
# number of repetitions for the Monte Carlo simulation
MC_iterations = 1e4 

#set flag below to TRUE if you want to create a grouped output (containing infomration for samples grouped together)
write_grouped_output = TRUE
#set flag below to TRUE if you want to create an individual output (containing information for each individual sample)
write_individual_output = TRUE
#set flag below to TRUE if you want to create an output concerning the uncertainty of the deconvolution step only (not calculating until final concentrations)
write_deconvolution_output = TRUE
#set flag below to TRUE if you want to create an output concerning the uncertainty of the whole calculation step (i.e. calculation until final concentration)
write_concentration_output = TRUE
```


#-----------------------------------NO USER INPUT NEEDED BELOW THIS POINT--------------------------------------

### libraries
```{r}
library(tidyr)#for data crunching
library(plyr)#for data crunching
library(dplyr)#for data crunching
library(ggplot2)#for plotting
library(data.table)#for data crunching
library(tictoc)#for timekeeping
library(openxlsx)#for xlsx reading and writing
```

### data import
```{r}
#importing the key matrix and its uncertainty
key_matrix = openxlsx::read.xlsx(input_key_xlxs, sheet="value matrix", startRow=2)
key_matrix.uncert = openxlsx::read.xlsx(input_key_xlxs, sheet = "uncertainty matrix", startRow=2)
key_matrix.isotope.info = openxlsx::read.xlsx(input_key_xlxs, sheet = "matrix isotope info", startRow=2)

#importing the data
peakareas = openxlsx::read.xlsx(input_data_xlxs, sheet = "peak areas", startRow=2)

#metadata about which isotopes were measured (we don't necessarily have the peak areas for all isotopes in the key matrix)
isotopes_measured = openxlsx::read.xlsx(input_data_xlxs, sheet = "isotopes_measured", startRow=2)$isotope
indices_isotopes_measured = which(key_matrix$isotope %in% isotopes_measured)

#by which isotope is to be normalized? Find the right row index. 
index_iso_norm = which(key_matrix.isotope.info[indices_isotopes_measured,]$Normalization_isotope==1)

#and information about sample and spike (volumes, etc. )
SaSpiMean = openxlsx::read.xlsx(input_data_xlxs, sheet = "Sample and Spike mean", startRow=2)
SaSpiUncert = openxlsx::read.xlsx(input_data_xlxs, sheet = "Sample and Spike uncertainty",startRow=2)

#importing the data uncertainties, if they exist. If nothing found, then return 0 as uncertainties.
peakareas.uncert = tryCatch(expr={openxlsx::read.xlsx(input_data_xlxs, sheet = "peak area uncertainties rel",startRow=2)},
                       error=function(e){
                         message("No uncertainties found for input data. Setting uncertainties to 0.")
                         return(peakareas %>% dplyr::mutate_all(function(x){0}))
                       })
peakareas.groupings = tryCatch(expr={openxlsx::read.xlsx(input_data_xlxs, sheet = "Sample groupings",startRow=2)},
                       error=function(e){
                         message("No groupings found for input data. Setting all groupings to 0.")
                         return(peakareas %>% dplyr::mutate_all(function(x){0}))
                       })

#convert everything into matrices for the calculation
peakareas.m = peakareas %>% as.matrix
peakareas.uncert.m = peakareas.uncert %>% as.matrix
key_matrix.m = key_matrix %>% dplyr::select(!isotope) %>% as.matrix
SaSpiMean.m = SaSpiMean[,-1]%>%as.matrix
SaSpiUncert.m = SaSpiUncert[,-1]%>%as.matrix
key_matrix.uncert.m = key_matrix.uncert %>% dplyr::select(!isotope)%>% as.matrix
peakareas.groupings.m = peakareas.groupings %>% as.matrix()
```

### testing for incongruencies in input data
```{r}
#a function that checks for the most typical errors. 
check_INPUT <- function(key_matrix.isotope.info, key_matrix, key_matrix.uncert, indices_isotopes_measured, 
                        peakareas, peakareas.uncert,peakareas.groupings, SaSpiMean.m, SaSpiUncert.m){
  if(length(indices_isotopes_measured)!= dim(key_matrix)[1]){
    warning("Less isotopes were measured than given in the key matrix. This may be wanted, but be aware.")
  }
  if(any(indices_isotopes_measured != sort(indices_isotopes_measured))){
    stop("The vector for measured isotopes must be given in ascending order. E.g: 198, 200, 201 is valid, 201,198,200 is NOT valid")
  }
  if (any(!(indices_isotopes_measured %in% 1:nrow(key_matrix)))){
    stop("Some of the selected isotopes (isotope) are not found in the key matrix!")
  }
  if (any(dim(key_matrix)!=dim(key_matrix.uncert))){
    stop("Different dimension for key matrix values and key matrix uncertainties")
  }
  if (sum(key_matrix.isotope.info$Normalization_isotope)!= 1){
    stop("Excactly one isotope must be chosen as normalization isotope. This should be indicated with a '1' in the column 'Normalization_isotope', all other isotopes should be marked with a '0'")
  }
  if (dim(key_matrix)[1] !=dim(key_matrix)[2]){
    stop("Key matrix is not quadratic, but it must be to compute the inverse!")
  }
  if (any(diag(key_matrix)==0)){
    stop("Key matrix has diagonal elements that are either 0. This is not allowed!")
  }
  if (any(is.na(key_matrix))){
    stop("Key matrix has missing/NaN elements. This is not allowed!")
  }
  if (any(is.na(key_matrix.uncert))){
    stop("Key matrix has missing/NaN elements. This is not allowed!")
  }
  if (dim(peakareas)[1] !=dim(key_matrix[indices_isotopes_measured, indices_isotopes_measured])[2]){
    stop("Dimension of input data is not coherent with dimension of key matrix")
  }
  if (dim(peakareas)[2] != dim(peakareas.uncert)[2]){
    stop("Dimension of peak area uncertainty sheet not coherent with dimensions of peak area sheet")
  }
  if (dim(peakareas)[2] != dim(peakareas.groupings)[2]){
    stop("Dimension of groupings sheet not coherent with dimensions of peak area sheet")
  }
  if (any(dim(SaSpiMean.m) != dim(SaSpiUncert.m))){
    stop("Dimension of Sample and Spike uncertainty does not coincide with dimension of Sample and Spike mean value")
  }
  
  return(message("Input data and key matrix dimensions are looking good. Proceeding."))
}
check_INPUT(key_matrix.isotope.info, key_matrix.m, key_matrix.uncert.m, indices_isotopes_measured, 
            peakareas.m, peakareas.uncert.m, peakareas.groupings.m, SaSpiMean.m, SaSpiUncert.m)
```




### check out the normalized key matrix, and the result we get by solving the system without any MC
```{r}
message("the normalized key matrix")
#creating a normalized key matrix (dividing each column by the diagonal element)
normfac.m = t(1/key_matrix.m[row(key_matrix.m)==col(key_matrix.m)]) %>% matrix(., nrow = length(.), ncol = length(.), byrow = TRUE)
key_matrix.norm.m = key_matrix.m*normfac.m
key_matrix.norm.m

message("solving without MC")
solved.m = solve(key_matrix.norm.m[indices_isotopes_measured,indices_isotopes_measured], peakareas.m)
solved.m
```

#------------------------------------------------------------------------------\

## Now the Monte Carlo (MC) simulation

### do the iterative step for the MC simulation. 
```{r}
key_trans.l = list()
SaSpi_trans.l = list()
data_trans.l = list()
deconvoluted.l=list()

tic() #timekeeping
for (k in 1:MC_iterations){
  set.seed(k)
  #transmute the key matrix
  key_trans.l[[k]] = rnorm(length(key_matrix.m), mean = key_matrix.m, sd = key_matrix.uncert.m) %>%
    matrix(nrow=dim(key_matrix.m)[1],ncol=dim(key_matrix.m)[2])
  
  #transmute the Spike mass, sample volume, etc.
  SaSpi_trans.l[[k]]=rnorm(length(SaSpiMean.m), mean = SaSpiMean.m, sd = SaSpiUncert.m)%>%
    matrix(nrow=dim(SaSpiMean.m)[1],ncol=dim(SaSpiMean.m)[2])
  
  #now create the transformed, normalized matrix (divide each column by the value of the diagonal element)
  temp.normfac.m = t(1/diag(key_trans.l[[k]])) %>% matrix(., nrow = length(.), ncol = length(.), byrow = TRUE)
  temp_key.norm.m = key_trans.l[[k]]*temp.normfac.m
  
  #transmute the peak area data
  data_trans.l[[k]] = rnorm(length(peakareas.m), mean = peakareas.m, sd = peakareas.m * peakareas.uncert.m) %>% #note the relative uncertainties
    matrix(nrow=dim(peakareas.m)[1],ncol=dim(peakareas.m)[2])
  
  #and finally solve. Note that we only take a subset of the matrix, if not all isotopes were measured
  deconvoluted.l[[k]] = solve(temp_key.norm.m[indices_isotopes_measured,indices_isotopes_measured], data_trans.l[[k]]) 
}#for
toc()#timekeeping

#I'm setting the FIRST element of the list as the not MC-modified calculation, this makes things easier later on. I'm losing 1 MC iteration, but who cares when you do a few thousand.
deconvoluted.l[[1]] = solved.m %>% as.matrix()
key_trans.l[[1]] = key_matrix.m
```

### The deconvolution is done and saved in the list "deconvoluted.l". But we still have to get concentrations from the deconvoluted signal
```{r}
#first we need some helper data. We need that for each MC iteration.
molarweight.mix.m.l = list()
abundance.m.l = list()
SpikeMass.l = list()
SampleV.l = list()

for (k in 1:MC_iterations){
  molarweight.mix.m =  matrix(nrow = nrow(key_matrix.isotope.info),ncol=1) #the molar weight of each "source" (spike, natural, etc.)
  for (i in 1:nrow(molarweight.mix.m)){
    molarweight.mix.m[i,1] = sum(key_trans.l[[k]][,i]/100*key_matrix.isotope.info$MM)
  }
  molarweight.mix.m.l[[k]] = molarweight.mix.m
  
  abundance.m.l[[k]] = diag(key_trans.l[[k]]) #the abundance of the major isotope in each "source" (Spike, natural, etc)

  SpikeMass.l[[k]] = SaSpi_trans.l[[k]][2,] * SaSpi_trans.l[[k]][3,]/molarweight.mix.m.l[[k]][indices_isotopes_measured,][index_iso_norm]
  SampleV.l[[k]] = SaSpi_trans.l[[k]][1,]
}##k


#now finally calculating the concentrations
concentrations.l = deconvoluted.l
for (k in 1:MC_iterations){ #loop through all list items, each item corresponding to a MC iteration
  for (j in 1:ncol(concentrations.l[[1]])){ # loop through each column, each column corresponding to a sample
    for (i in 1:nrow(concentrations.l[[1]])){ #loop through each row. Each row corresponding to an isotope.
      concentrations.l[[k]][i,j] = (deconvoluted.l[[k]][i,j]/abundance.m.l[[k]][indices_isotopes_measured][i])/(deconvoluted.l[[k]][index_iso_norm,j]/abundance.m.l[[k]][indices_isotopes_measured][index_iso_norm])*molarweight.mix.m.l[[k]][indices_isotopes_measured][i]*SpikeMass.l[[k]][j]/SampleV.l[[k]][j]
    }#i loop
  }#j loop
}#k loop
```

### now glue the results together and calculate overview statistics
```{r}
#I write a function to avoid repetition.
summarise_results = function(input.l, peakareas, indices_isotopes_measured, MC_iterations){
  input.df = do.call(rbind, input.l)%>%
    as.data.frame()%>%
    dplyr::rename_at(names(.),~names(peakareas))%>% #recover initial names
    dplyr::mutate(varname = rep(paste0("isotope",indices_isotopes_measured[1]:tail(indices_isotopes_measured,1)),MC_iterations),
                  MC_iteration = rep(1:MC_iterations,dim(peakareas.m)[1])%>%sort)
  
  input.df.long = input.df %>%
    pivot_longer(1:ncol(peakareas))%>%
    dplyr::full_join(.,peakareas.groupings%>%as.data.frame%>%pivot_longer(1:ncol(peakareas), values_to="group"))
  
  
  #calculate overview statistics for the individual output (each sample treated differently)
  input.df_individual.s = input.df%>%
    dplyr::group_by(varname)%>%
    dplyr::summarise(across(.cols = contains("Sample"), .fns = list(mean =~mean(.,na.rm=TRUE),
                                                                    uncert_k2 =~2*sd(.,na.rm=TRUE),
                                                                    sd =~sd(.,na.rm=TRUE),
                                                                    rel_sd =~sd(.,na.rm=TRUE)/mean(.,na.rm=TRUE),
                                                                    median =~median(.,na.rm=TRUE),
                                                                    iqr =~IQR(.,na.rm=TRUE),
                                                                    q025 =~quantile(.,0.025),
                                                                    q975 =~quantile(.,0.975))))
  
  #calculate overview statistics for the grouped output (group certain samples together)
  input.df_groups.s = input.df.long%>%
    dplyr::group_by(varname,group)%>%
    dplyr::summarise(across(.cols = "value", .fns = list(mean=~mean(.,na.rm=TRUE),
                                                         uncert_k2 =~2*sd(.,na.rm=TRUE),
                                                         sd = ~sd(.,na.rm=TRUE),
                                                         rel_sd =~sd(.,na.rm=TRUE)/mean(.,na.rm=TRUE),
                                                         median=~median(.,na.rm=TRUE),
                                                         iqr =~IQR(.,na.rm=TRUE),
                                                         q025 =~quantile(.,0.025),
                                                         q975 =~quantile(.,0.975))))
  
  return(list(input.df_individual.s, input.df_groups.s))
}#summarise_results


results_deconvolution = summarise_results(deconvoluted.l, peakareas, indices_isotopes_measured, MC_iterations)
results_concentrations = summarise_results(concentrations.l, peakareas, indices_isotopes_measured, MC_iterations)

# results_deconvolution[[1]]
# results_deconvolution[[2]]
```

#----------------------------------------------------------------------------------------\

# All calculations are done. Now passing to the output

### create metadata to store in files
```{r}
metadata = "metadata"
attr(metadata, "Created the") = as.character(Sys.time())
attr(metadata,"By user") =Sys.info()[7]
attr(metadata,"Operating System") = Sys.info()[1]
attr(metadata,"MC iterations") = as.character(MC_iterations)
attr(metadata, "Input data file") = input_data_xlxs
attr(metadata, "Input key file") = input_key_xlxs

now = Sys.time()
timestamp = format(now, "%Y%m%d_%H%M%S")
```

### writing the individidual output files
```{r}
#a function to write the results for individual (per sample, not grouped) output
write_output_individual = function(fname_out, df_individual.s, results.l){
  wb=createWorkbook()
  addWorksheet(wb=wb, sheetName="Metadata")
  writeData(wb, sheet="Metadata", attributes(metadata)%>%as.matrix, colName=FALSE, rowNames=TRUE)
  
  #add the non-MC results. This information is contained in the argument "results.l"
  addWorksheet(wb, sheetName="solved-noMC")
  writeData(wb, sheet="solved-noMC", results.l[[1]]%>%as.data.frame())
  
  for (k in c("mean","uncert_k2","sd","rel_sd","median","iqr","q025","q975")){
    tempout = df_individual.s %>% dplyr::select(contains(k))%>%
      dplyr::rename_all(function(x){gsub(paste0("_",k),"",x)})
    
    addWorksheet(wb, sheetName=paste0("MC-",k))
    writeData(wb, sheet=paste0("MC-",k), tempout)
  }
  saveWorkbook(wb, fname_out,overwrite=TRUE)
}#write_output_individual


#check which output to be written and write. 
if (write_deconvolution_output & write_individual_output){
  write_output_individual(fname_out = paste0(here::here(),"/data_output/MC_output_devonvolutions_individual_",timestamp,".xlsx"), 
                 results_deconvolution[[1]], deconvoluted.l)
}
if (write_concentration_output & write_individual_output){
  write_output_individual(fname_out = paste0(here::here(),"/data_output/MC_output_concentrations_individual_",timestamp,".xlsx"),
                 results_concentrations[[1]], concentrations.l)
}
```

### writing the grouped output files
```{r}
#a function to write the results for grouped samples.
write_output_grouped = function(fname_out, df_grouped.s){
  wb=createWorkbook()
  addWorksheet(wb, sheetName="Metadata")
  writeData(wb, sheet="Metadata", attributes(metadata)%>%as.matrix,colNames=FALSE, rowNames=TRUE)
  
  for (k in unique(df_grouped.s$group)){
    tempout = df_grouped.s %>% dplyr::filter(group==k)%>%dplyr::select(!group)
    
    addWorksheet(wb, sheetName=paste0("group-",k))
    writeData(wb, sheet=paste0("group-",k),tempout)
  }
  saveWorkbook(wb, fname_out, overwrite=TRUE)
}#write_output_grouped

#check which output to be written and write. 
if (write_deconvolution_output & write_grouped_output){
  write_output_grouped(fname_out = paste0(here::here(),"/data_output/MC_output_deconvolutions_groups_",timestamp,".xlsx"), 
                     results_deconvolution[[2]])
}
if (write_concentration_output & write_grouped_output){
  write_output_grouped(fname_out = paste0(here::here(),"/data_output/MC_output_concentrations_groups_",timestamp,".xlsx"),
                     results_concentrations[[2]])
}
```
